# 第三天

## 1，什么是集成学习算法？
集成学习算法是通过构建并结合多个机器学习器来完成学习任务，也就是通常说的“博采众长”。能在机器学习算法中拥有较高的准确率，缺点是模型的训练过程可能比较复杂，效率不是很高。


## 2，集成学习主要有哪几种框架, 并简述它们的工作过程？
主要有bagging，boosting
bagging算法（装袋法）是bootstrap aggregating的缩写，它主要对样本训练集合进行随机化抽样，通过反复的抽样训练新的模型，最终在这些模型的基础上取平均。
boosting算法（提升算法）是常用的有效的统计学习算法，属于迭代算法，它通过不断地使用一个弱学习器弥补前一个弱学习器的“不足”的过程，来串行地构造一个较强的学习器，这个强学习器能够使目标函数值足够小。


## 3，Boosting算法有哪两类，它们之间的区别是什么？
Boosting算法主要有AdaBoost算法和Gradient Boosting算法。
AdaBoost（adaptive boosting + 单层决策树）的基本思想是：通过训练数据的分布构造一个分类器，然后通过误差率求出这个弱分类器的权重，然后通过更新训练数据的分布，迭代进行，直到迭代次数或者损失函数小于某一阈值。
Gradient Boosting算法的基本思想是：串行地生成多个弱学习器，每个弱学习器的目标是拟合先前累加模型的损失函数的负梯度，使加上该弱学习器后的累积模型损失往负梯度的方向减少。


## 4，什么是偏差和方差？
偏差（Bias）描述的是预测值和真实值之差，而方差（Variance）描述的是预测值作为随机变量的离散程度。


## 5，为什么说Bagging可以减少弱分类器的方差，而Boosting 可以减少弱分类器的偏差？
集成学习中的基模型是弱模型，通常来说弱模型是偏差高但是方差小的模型。

Bagging对样本重采样，对每一重采样得到的子样本训练一个模型，最后取平均。

bagging中的基模型为强模型（偏差低，方差高），而Boosting的基模型为弱模型（偏差高，方差低），
Bagging后的bias和单个子模型接近，一般来说不能显著降低bias｡另一方面，苦名子模型独立，则可以显著降低variance,若各子模型完全相同，则不会降低variance
Boosting由于租用sequential,adaptive的策略，各子模型之间是强相关的，千里子模型之和并不能显著降低variance



## 6，简述一下随机森林算法的原理
随机森林是指，从原台训练样本集中有放回地重复随机抽取k样本生成新的训东样本集合，然后根据助样本集生成k个分类树组成随机森林，新数据的分类结
果按分类树投票多少形成的分数而定



## 7，随机森林的随机性体现在哪里？
将多个决策树合在一起每棵树的建立依赖干个独立抽取
的样品。单棵树的分类能力可能很小，但在随机产生大量的决策权后，一个测试样品可以通过每一棵树的分类结果经统计后选择最可的分类能
