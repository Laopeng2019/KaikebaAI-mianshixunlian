# 第四天

## 1，简述GBDT原理。
 GBDT全称“梯度提升决策树”（Gradient Boosting Decision Tree），是一种集成模型，采用的基础模型是CART回归树。
y=f(x) + residual
评价一个模型的预测能力，一般考察残差的两个方面：（1）偏差，即与真实值分布的偏差大小；（2）方差，体现模型预测能力的稳定性，或者说鲁棒性。

GBDT用了多模型集成的策略，针对残差进行拟合，进而降低模型的偏差和方差。

 集成模型（Ensemble Model）不是一种具体的模型，而是一种模型框架，它采用的是“三个臭皮匠定个诸葛亮”的思想。将若干个模型（弱学习器）按照一定的策略组合起来，共同完成一个任务，形成特定的组合策略，可以帮助集成模型降低预测的偏差或者方差。

 
 
## 2，GBDT常用损失函数有哪些？
 针对回归，有直接用连续的值计算负梯度，有均方差、绝对损失、huber损失、分位数损失
针对分类，有指数（Adaboost）、对数
 
 
## 3，GBDT如何用于分类?
 使用CART分类树，即可用于分类，主要区别是，样本分组时，评价（特征，取值）的质量指标为gini系数
 
 
## 4，为什么GBDT不适合使用高维稀疏特征?
因为高维稀疏模型会导致决策树的建立非常困难，从而训练非常困难，而且容易过拟合
 
 
## 5，GBDT算法的优缺点？
优点：
可以灵活处理各类数据，包括连续值和离散值
在相对少的调参时间情况下，预测的准确率也可以比较高，相对SVM来说
使用一些强壮的损失函数，对异常值的鲁棒性非常强。比如Huber损失函数和Quantile损失函数。



缺点：
由于弱学习器之间存在依赖关系，难以并行训练数据。
在高维稀疏数据上，表现不如SVM或者神经网络
在处理文本分类特征的问题上，相对其他模型的优势不如在处理数值特征时明显
训练过程需要串行，只能在决策树内部采用一些局部并行手段提高训练速度
 
 
## 6，简述XGBoost。
 XGBoost是大规模并行boosting tree的工具，是目前最快最好的开源boosting tree工具包，比常见的包快10倍以上。不仅支持决策树，还支持线性模型。
 
 
 
## 7，XGBoost和GBDT有什么不同？
一是算法本身：在算法的弱学习器模型选择上，对比GBDT只支持决策树，还可以增加很多其他的弱学习器。最大的不同就是目标函数的定义，XGBoost对损失函数进行了二阶泰勒展开，同时使用了一阶和二阶导数，更加准确。
二是算法运行效率的优化：对每个弱学习器，比如决策树建立的过程做并行选择，找到合适的子树分裂特征和特征值。在并行选择之前，先对所有的特征的值进行排序分组，方便前面说的并行选择。对分组的特征，选择适合的分组大小，使用CPU缓存进行读取加速。将各个分组保存到多个硬盘以提高IO速度。

三是算法鲁棒性的优化：对于缺失值的特征，通过枚举所有缺失值在当前节点是进入左子树还是右子树来决定缺失值的处理方法。算法本身加入了L1和L2正则化，可以防止过拟合，泛华能力更强


