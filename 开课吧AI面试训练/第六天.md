# 第六天

## 1， SVM算法的优缺点

### 优点
- 有严格的数学理论支持，可解释性强，不依靠统计方法，从而简化了通常的分类和回归问题
- 能找出对人物至关重要的关键样本（即支持向量）
- 采用核技巧之后，可以处理非线性分类/回归任务
- 最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，在这某种意义上避免了“维数灾难”

###缺点
- 训练时间长。当采用SMO算法时，由于每次都需要挑选一对参数，因此时间复杂度为$O(N^2)$，其中N为训练样本的数量；
- 当采用核技巧时，如果需要存储核矩阵，则空间复杂度为$O(N^2)$；
- 模型预测时，预测时间与支持向量的个数成正比。当支持向量的数量较大时，预测计算复杂度较高。

## 2，SVM的超参数C如何调节

增加软间隔之后的优化目标变成了：

$$
    z = \min_w \cfrac{1}{2}||w||^2+C\sum_{i=1}^m \xi_i, z\rightarrow\text{最小}
$$

$$
    \text{s.t.}\quad g_i(w,b)=1-y_i(w^T x_i+b)\leq 0,\quad i=1,2...,n
$$

C是一个大于0的常数，可以理解为错误样本的惩罚程度，若C为无穷大，$\xi^2$必然无穷小，如此一来线性SVM就又变成了线性可分SVM；当C为有限值的时候，才会允许部分样本不遵循约束条件。


## 3，SVM的核函数如何选择

对于在有限维度向量空间中线性不可分的样本，将其映射到更高维度的向量空间里，再通过间隔最大化的方式，得到支持向量机，就是非线性SVM。
但是因为低维空间映射到高维空间后维度可能会很大，如果将全部样本的点乘都计算好了，这样的计算量太大了。
\
但如果由这样的核函数$k(x,y)=(\phi (x_i)\cdot(x_j))$，$x_i$与$x_i$在特征空间的内积等于它们在原始样本空间中通过函数$k(x,y)$计算的结果，我们就不需要计算高维甚至无穷维空间的内积了。
\
常见的核函数有：
线性核函数(Linear核)
$$
    k(x_i,x_i)=x_i^Tx_j    
$$

多项式核函数(Polynomial核)
$$
    k(x_i,x_i)=(x_i^Tx_j)^d
$$

高斯核函数(RBF核/径向基核)
$$
    k(x_i,x_i)=\exp(-\cfrac{||x_i - x_j||}{2\delta ^2})
$$
- 如果Feature的数量大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM
- 如果Feature的数量比较小，而且样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel
- 如果Feature的数量比较小，而且样本数量很多，需要手工添加一些feature变成第一种情况。也就是说，如果特征维数很高，往往线性可分（SVM解决非线性分类问题的思路就是将样本映射到更高维的特征空间中），可以采用LR或者线性核的SVM；
- 如果样本数量很多，由于求解最优化问题的时候，目标函数涉及两两样本计算内积，使用高斯核明显计算量会大于线性核，所以手动添加一些特征，使得线性可分，然后可以用LR或者线性核的SVM；
- 如果不满足上述两点，即特征维数少，样本数量正常，可以使用高斯核的SVM。
  



## 4，简述SVM硬间隔推导过程

$$
    \hat{\gamma}_i= y_i(w\cdot x_i+b)
$$

同样，这里定义最小的函数间隔为$\hat{\gamma}_i$:

$$
    \hat{\gamma}= \min_{i=1,...,N}\hat{\gamma}_i
$$


## 5，简述SVM软间隔推导过程




## 6，简述决策树的构建过程

- 节点的分裂：一般当一个节点所代表的属性无法给出判断时，则选择将这一节点分成2个子节点（二叉树以外的情况会分成n个子节点）
- 阈值的确定：选择适当的阈值使得分类错误率最小

## 7，ID3决策树与C4.5决策树的区别
### ID3: 
由增熵(Entropy)原理来决定哪个做父节点，哪个节点需要分裂。对于一组数据来说，熵越小说明信息杂乱度越小，分类效果越好。熵定义如下：
$$
    \text{Entropy}=-\sum(P(x_i)*\log_2(P(x_i)))
$$
其中$P(x_i)$为$x_i$出现的概率。当Entropy最大为1的时候，是分类效果最差的状态，当它最小为0的时候，是完全分类的状态。因为熵等于0是理想状态，一般实际情况下，熵介于0到1之间。

### C4.5:
ID3分割太细了，训练数据的分类可以达到0错误率，但是因为新的数据核训练数据不同，所以面对新的数据分错率反倒上升了。决策树是通过分析训练数据，得到数据的统计信息，而不是专门训练数据量身定做的。C4.5中，优化项要除以分割太细的代价，这个比值叫信息增益率，显然分割太细的分母增加，信息增益率会降低。

信息增益率：
$$
    \text{GainRatio}(D|A) = \cfrac{\text{infoGain}(D|A)}{\text{IV}(A)}
$$

$$
    \text{IV}(A) = -\sum_{k=1}^{K}\cfrac{|D_k|}{|D|} * \log_2 \cfrac{|D_k|}{|D|}
$$
