# 第六天

## 1， SVM算法的优缺点

### 优点
- 有严格的数学理论支持，可解释性强，不依靠统计方法，从而简化了通常的分类和回归问题
- 能找出对人物至关重要的关键样本（即支持向量）
- 采用核技巧之后，可以处理非线性分类/回归任务
- 最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，在这某种意义上避免了“维数灾难”

###缺点
- 训练时间长。当采用SMO算法时，由于每次都需要挑选一对参数，因此时间复杂度为$O(N^2)$，其中N为训练样本的数量；
- 当采用核技巧时，如果需要存储核矩阵，则空间复杂度为$O(N^2)$；
- 模型预测时，预测时间与支持向量的个数成正比。当支持向量的数量较大时，预测计算复杂度较高。

## 2，SVM的超参数C如何调节

增加软间隔之后的优化目标变成了：
$$
    z = \min_w \cfrac{1}{2}||w||^2+C\sum_{i=1}^m \xi_i,z\rightarrow\text{最小}

$$

$$
    \text{s.t.}\quad g_i(w,b)=1-y_i(w^T x_i+b)\leq 0,\quad i=1,2...,n
$$

C是一个大于0的常数，可以理解为错误样本的惩罚程度，若C为无穷大，$\xi^2$必然无穷小，如此一来线性SVM就又变成了线性可分SVM；当C为有限值的时候，才会允许部分样本不遵循约束条件。


## 3，SVM的核函数如何选择

对于在有限维度向量空间中线性不可分的样本，将其映射到更高维度的向量空间里，再通过间隔最大化的方式，得到支持向量机，就是非线性SVM。
但是因为低维空间映射到高维空间后维度可能会很大，如果将全部样本的点乘都计算好了，这样的计算量太大了。
\
但如果由这样的核函数$k(x,y)=(\phi (x_i)\cdot(x_j))$，$x_i$与$x_i$在特征空间的内积等于它们在原始样本空间中通过函数$k(x,y)$计算的结果，我们就不需要计算高维甚至无穷维空间的内积了。
\
常见的核函数有：
线性核函数(Linear核)
$$
    k(x_i,x_i)=x_i^Tx_j    
$$

多项式核函数(Polynomial核)
$$
    k(x_i,x_i)=(x_i^Tx_j)^d
$$

高斯核函数(RBF核/径向基核)
$$
    k(x_i,x_i)=\exp(-\cfrac{||x_i - x_j||}{2\delta ^2})
$$
- 如果Feature的数量大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM
- 如果Feature的数量比较小，而且样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel
- 如果Feature的数量比较小，而且样本数量很多，需要手工添加一些feature变成第一种情况。也就是说，如果特征维数很高，往往线性可分（SVM解决非线性分类问题的思路就是将样本映射到更高维的特征空间中），可以采用LR或者线性核的SVM；
- 如果样本数量很多，由于求解最优化问题的时候，目标函数涉及两两样本计算内积，使用高斯核明显计算量会大于线性核，所以手动添加一些特征，使得线性可分，然后可以用LR或者线性核的SVM；
- 如果不满足上述两点，即特征维数少，样本数量正常，可以使用高斯核的SVM。
  



## 4，简述SVM硬间隔推导过程




## 5，简述SVM软间隔推导过程


## 6，简述决策树的构建过程


## 7，ID3决策树与C4.5决策树的区别



